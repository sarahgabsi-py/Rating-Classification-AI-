# -*- coding: utf-8 -*-
"""PROGETTO_IA_RATING_CLASS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cXgKRoGaFezrTY3UwwVMNA9XMPSbEAJZ
"""

!pip install --upgrade transformers

!pip install datasets

!pip install torch

!pip install pandas

!pip install matplotlib seaborn

"""**PROGETTO IA: CLASSIFICAZIONE DEL RATING**

----------------------------------

Questo progetto ha come obiettivo la classificazione automatica del punteggio (da 1 a 5 stelle) associato alle recensioni di libri, utilizzando il dataset Amazon Data Science Book Reviews. La domanda di ricerca che guida il lavoro è la seguente:

"In che modo il fine-tuning di un modello di linguaggio pre-addestrato può migliorare le prestazioni nella previsione del rating di una recensione testuale?"

Attraverso l’adattamento (fine-tuning) del modello DistilBERT-base-uncased su un corpus di recensioni reali, il progetto intende dimostrare come i modelli Transformer possano apprendere le sfumature linguistiche che distinguono una recensione da 1 stella rispetto a una da 5, superando le tradizionali tecniche di sentiment analysis binaria.

**IL DATASET:**
-------------------------------
Questo dataset contiene 20.647 recensioni Amazon per 836 libri di data science. Ogni recensione è composta dal testo e dal rating (numero di stelle da 1 a 5).

# **FASE 1: DATA PRE-PROCESSING**
-----------------------------------
**Caricamento e preparazione dei dati:**

1. Carico il dataset

2. Analisi esplorativa

3. Divisione in train, validation e test

4. Pulizia dati

5. Analisi della lunghezza delle recensioni

6. Bilanciamento delle classi tecnica di undersampling

**Step 1:** carico a preparo il dataset

In questa fase iniziale,  carico il dataset. Successivamente converto il dataset in un dataframe di pandas per una manipolazione più semplice e intuitiva.
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

#carico il file csv in una dataframe
df_reviews = pd.read_csv('Amazon Reviews Science.csv')

#visualizzo le prime righe del dataframe
df_reviews.head()

"""**STEP 2:** Analisi esplorativa

Qui, calcolo la distribuzione del rating (starts) del dataset e creo un grafico a barre per visualizzare tale distribuzione. Questo passaggio mi aiuterà a:

- Capire la struttura del dataset

- Identificare lo sbilanciamento delle classi, che è un problema molto comune nei dataset di recensioni.

- Guidare le scelte di preprocessing e training
"""

import seaborn as sns
import matplotlib.pyplot as plt

#conta le recensioni per ogni punteggio
rating_count= df_reviews['stars'].value_counts().sort_index()

#Visualizzazione grafica della distribuzione
sns.barplot(x=rating_count.index, y=rating_count.values, palette="mako")
plt.title('Distribuzione del rating (1-5 stelle)')
plt.xlabel('Rating')
plt.ylabel('Numero di recensioni')
plt.show()

"""Analizzando il grafico, il dataset presenta un numero maggiore di recensioni a 5 stelle evidenziando una distribuzione delle classi non uniforme cioè le classi non sono bilanciate.

**STEP 3:** Divisione in Training set, Validation e Test

Qui l'obiettivo è dividere il datset in 3 sottoinsiemi. Questo passaggio è fondamentale per garantire che il modello venga addestrato, validato e testato in modo corretto e imparziale:

- Train set: usato per addestrare il modello

- Validation set: usato per ottimizzare i parametri e prevenire overfitting

- Test set: usato per valutare le performance finali del modello su dati mai visti
"""

from sklearn.model_selection import train_test_split

# 1) Split iniziale stratificato per ottenere 80/10/10
train_val, test = train_test_split(
    df_reviews,
    test_size=0.10,
    stratify=df_reviews['stars'],
    random_state=42
)

train, val = train_test_split(
    train_val,
    test_size=0.1111111111,   #  10% del totale
    stratify=train_val['stars'],
    random_state=42
)

print('Dimensioni del set di addestramento:',train.shape)
print('Dimensioni del set di validazione:',val.shape)
print('Dimensioni del set di test:',test.shape)

"""**STEP 4:** Pulizia dei dati e conversione del rating 0-4 (1-5 stars)"""

# assegno label prima a tutti gli split (-1 perche il modello richiede la conversione delle etichette 0-4)
train['label'] = train['stars'].astype(int) - 1
val['label']   = val['stars'].astype(int) - 1
test['label']  = test['stars'].astype(int) - 1

# Pulizia SOLO sul train (esempio: rimuovo commenti vuoti e testi <= 1 parola)
train_clean = train[train['comment'].notnull() & (train['comment'].str.strip() != '')].copy()
train_clean['n_words'] = train_clean['comment'].apply(lambda x: len(str(x).split()))
train_clean = train_clean[train_clean['n_words'] > 1].copy()

print("Train per classe:\n", train['label'].value_counts().sort_index())
print("Val per classe:\n",   val['label'].value_counts().sort_index())
print("Test per classe:\n",  test['label'].value_counts().sort_index())

"""**STEP 4:** Analisi della lunghezza delle recensioni

Qui, analizzo la lunghezza del testo che mi aiuterà a scegliere la lunghezza massima per la tokenizzazione (es:max_lenght= 126 o 256)
"""

# Aggiungi colonna con lunghezza del testo
df_reviews['review_length'] = df_reviews['comment'].apply(lambda x: len(str(x).split()))

# Statistiche
print(df_reviews['review_length'].describe())

# Visualizzazione
sns.histplot(df_reviews['review_length'], bins=50, kde=True, color='teal')
plt.title("Distribuzione della lunghezza delle recensioni")
plt.xlabel("Numero di parole")
plt.ylabel("Frequenza")
plt.show()

"""Interpretazione dei risultati

- count: (numero totale di recensioni analizzate) 20.647

- mean: (lunghezza media delle recensioni) ogni recensione ha in media 87.5 parole

- std: (deviazione standard) c'è molta variabilità, alcune recensioni sono molto più lunghe o più corte

- min:	(lunghezza minima)	la recensione più corta ha 1 parola. NB:probabilmente da filtrare

- 25%: il 25% delle recensioni ha meno di 20 parole

- 50%: il 50% ha meno di 46 parole (quindi la metà è piuttosto breve)

- 75%: il 75% ha meno di 101 parole

- max: (Lunghezza massima)	la recensione più lunga ha 4868 parole (molto probabilmente un outlier, ovvero un **valore anomalo**)             

**Considerazioni personali:** Nel dataset, la recensione più corta ha 1 parola e la più lunga ne ha 4868. Entrambe sono outlier rispetto alla distribuzione generale, e vanno valutate con attenzione in base all'obiettivo di questo progetto: classificare il rating di una recensione testuale usando DistilBERT.  Si procede eliminando le recensioni con una sola parola e lasciando la più lunga poichè verrà troncata durante il processione di tokenizzazione (impostando max_lenght=256 e truncation=True)

**STEP 5:** Bilanciamento delle classi

In questa fase si procede con il bilanciamento delle classi tramite la tecnica dell'Undersampling.
"""

# Undersampling del train fino alla classe minima presente nel train_clean
train_min = train_clean['label'].value_counts().min()
train_bal = train_clean.groupby('label', group_keys=False).apply(lambda g: g.sample(n=train_min, random_state=42)).reset_index(drop=True)

# Mantengo val e test senza bilanciamento
val_bal = val.copy()
test_bal = test.copy()

# Controllo finale: stampo i conteggi per classe
print("Train per classe:\n", train_bal['label'].value_counts().sort_index())
print("Val per classe:\n",   val_bal['label'].value_counts().sort_index())
print("Test per classe:\n",  test_bal['label'].value_counts().sort_index())

# Verifica dimensioni assolute
print("Totali -> train:", len(train_bal), " val:", len(val_bal), " test:", len(test_bal))

"""- Il train_bal ha 741 esempi per ciascuna classe → quindi è bilanciato tramite undersampling.

- val e test mostrano distribuzioni sbilanciate, con molte più recensioni nella classe 4  perché validation e test devono riflettere la distribuzione reale.

Controlli per valutare i conteggi per classe nei vari split
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Aggiungo una colonna 'split' per identificare l'origine di ciascuna riga
train_plot = train_bal.assign(split='train')
val_plot   = val_bal.assign(split='val')
test_plot  = test_bal.assign(split='test')

# Unisco i tre set in un unico DataFrame per la visualizzazione
combined = pd.concat([train_plot, val_plot, test_plot], ignore_index=True)


# Grafico: barre affiancate per label, separate per split
plt.figure(figsize=(10,5))
sns.countplot(x='label', hue='split', data=combined, palette='mako',order=sorted(combined['label'].unique()))
plt.title('Distribuzione delle classi per split dopo l\'undersampling')
plt.xlabel('Label (0 = 1 stella, 4 = 5 stelle)')
plt.ylabel('Numero di esempi')
plt.legend(title='Split')
plt.tight_layout()
plt.show()

#elimino eventuali valori nulli rimasti
train_bal = train_bal.dropna(subset=["label"])

"""# **FASE 2:PROCESSING**


**TOKENIZZAZIONE, CREAZIONE DEI TENSORI E DEFINIZIONE DEL MODELLO PER L'ADDESTRAMENTO**


--------------------------------
Questa fase è caratterizzata dai seguenti passaggi:

1. tokenizzazione e preparazione dei dati

2. creazione dei tensori di input

3. definizione del modello e addestramento

4. Valutazione sul test set

5. Analisi eventuali errori (anomalie)

**STEP 1:** Tokenizzazione e preparazione dei dati

La tokenizzazione è il processo che trasforma il testo in numeri (token), cioè in una sequenza che il modello può comprendere. DistilBERT non legge parole, ma ID numerici associati a sottoparti di parole (subword tokens). Il processo si divide nelle seguenti fasi:

- importare e caricare il tokenizer (Autotokenizer: riconosce automaticamente il tokenizer corretto in base al nome del modello. Caricando distil-bert-uncased, viene usato DistilBertTokenizerFast internamente)

- definire la funzione di tokenizzazione

- applicare la funzione ai tre set: training, validation e test

L'obiettivo è convertire le recensioni testuali in input numerici (input_ids, attention_mask) compatibili con il modello DistilBERT, usando un tokenizer automatico e flessibile.
"""

#importo e carico il tokenizer
from transformers import AutoTokenizer

tokenizer= AutoTokenizer.from_pretrained('distilbert-base-uncased')

# Funzione di tokenizzazione in batch che restituisce il dizionario tokenizzato
def batch_tokenize_texts(texts, tokenizer, max_length=256):
    tokenized = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_attention_mask=True,
        return_tensors=None
    )
    # tokenized è un dict con liste per 'input_ids' e 'attention_mask'
    return tokenized

#applico la funzione ai set
# testi come liste
train_texts = train_bal['comment'].astype(str).tolist()
val_texts   = val_bal['comment'].astype(str).tolist()
test_texts  = test_bal['comment'].astype(str).tolist()

# tokenizzazione batch
train_tok = batch_tokenize_texts(train_texts, tokenizer, max_length=256)
val_tok   = batch_tokenize_texts(val_texts,   tokenizer, max_length=256)
test_tok  = batch_tokenize_texts(test_texts,  tokenizer, max_length=256)

# verifica rapida
print("Esempi tokenizzati:", len(train_tok['input_ids']), len(val_tok['input_ids']), len(test_tok['input_ids']))
print("Lunghezza input_ids esempio:", len(train_tok['input_ids'][0]))

print(train_tok.keys())
print(val_tok.keys())
print(test_tok.keys())

"""tre dizionari con:
- "input_ids": una lista di liste di interi, dove ogni sottolista rappresenta i token numerici di una recensione.

- "attention_mask": una lista di liste di 0 e 1, che indica quali token sono reali e quali sono padding.
"""

print(tokenizer.decode(train_tok['input_ids'][0]))
print(train_tok['attention_mask'][0])

print(tokenizer.decode(val_tok['input_ids'][0]))
print(val_tok['attention_mask'][0])

print(tokenizer.decode(test_tok['input_ids'][0]))
print(test_tok['attention_mask'][0])

"""**STEP 2:** Creazione dei tensori di input

Qui, vengono creati i tensori di input con l'obiettivo di associare gli input tokenizzati (input_ids, attention_mask) alle etichette (label) e impacchettarli in un dataset compatibile con PyTorch o Hugging Face Trainer:

- conversione delle etichette in tensori: perchè il modello ha bisogno che le etichette siano in formato numerico. Questo rende possibile calcolare la funzione di perdita (CrossEntropyLoss) durante il training
-  creazione dei dataset da utilizzare nell’addestramento, validazione e test del modello,  utilizzando la classe Dataset della libreria Hugging Face datasets. Questa scelta è motivata dalla piena compatibilità con il modello DistilBERT, già integrato nella piattaforma Hugging Face. Dopo aver effettuato la tokenizzazione dei testi e convertito le etichette in tensori PyTorch, ho associato le etichette (labels) direttamente ai dizionari contenenti gli input tokenizzati (input_ids e attention_mask). Successivamente, ho utilizzato il metodo Dataset.from_dict() per creare tre oggetti dataset distinti: train_dataset, val_dataset e test_dataset.

- istanzio i dataset
"""

from datasets import Dataset

# Costruisco i dizionari con input_ids, attention_mask e label
train_dict = {
    "input_ids": train_tok["input_ids"],
    "attention_mask": train_tok["attention_mask"],
    "label": train_bal["label"].astype(int).tolist()
}

val_dict = {
    "input_ids": val_tok["input_ids"],
    "attention_mask": val_tok["attention_mask"],
    "label": val_bal["label"].astype(int).tolist()
}

test_dict = {
    "input_ids": test_tok["input_ids"],
    "attention_mask": test_tok["attention_mask"],
    "label": test_bal["label"].astype(int).tolist()
}

# Creo i dataset con il metodo dataset.from_dict()
train_dataset = Dataset.from_dict(train_dict)
val_dataset   = Dataset.from_dict(val_dict)
test_dataset  = Dataset.from_dict(test_dict)

# Imposto il formato torch per Trainer
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# Verifica rapida
print("Train:", len(train_dataset), "Val:", len(val_dataset), "Test:", len(test_dataset))
print("Esempio train:", train_dataset[0])

"""Ogni esempio è un dizionario con input_ids, attention_mask e label, già convertito in tensori PyTorch, così ogni esempio ha:

- gli input tokenizzati:'input_ids'
È una lista di token numerici che rappresentano la recensione. Inizia con [101] (token [CLS]) e termina con [102] (token [SEP]), come previsto. I numeri successivi sono token subword generati dal tokenizer.I tanti 0 alla fine sono padding, perché la recensione è più corta di max_length=256.

- la maschera di attenzione: È una lista di 1 e 0:

  - 1 indica token reali (da [CLS] fino a [SEP])
  - 0 indica token di padding

- la classe associata: (labels)il valore che rappresenta la classe associata a quella recensione.

**STEP 3:** Definizione del modello e addestramento

In questa fase l'obiettivo è di:
- Caricare il modello DistilBertForSequenceClassification

- Impostare il numero di classi (5, da 0 a 4)

- Configurare il Trainer con parametri di addestramento

- Preparare il training e la validazione
"""

from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import accuracy_score

#carico il modello DistilBertForSequenceClassification e imposto il numero di classi
model= DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=5)

#configurazione dei parametri di addestramento
training_args= TrainingArguments(
    output_dir='./results',            #salvare il modello
    eval_strategy="epoch",             #valuta alla fine di ogni epoca
    save_strategy='epoch',             #salva alla fine di ogni epoca
    num_train_epochs=3,                #numero epoche 3 per evitare overfitting
    per_device_train_batch_size=8,     #batch bilanciato per stabilità
    per_device_eval_batch_size=8,
    metric_for_best_model='accuracy',  #accuracy come riferimento
    logging_strategy="steps",
    logging_steps=50,
    report_to="none"	                 #disattivare logging esterni
)


#definisco le metriche di valutazione
def compute_metrics(pred):
  labels= pred.label_ids
  preds=np.argmax(pred.predictions,axis=1)
  accuracy=accuracy_score(labels,preds)
  return{'accuracy':accuracy_score(labels,preds)}


#creo il trainer
trainer=Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

#avvio l'addestramento
trainer.train()

"""**STEP 4:** Valutazione sul test set

Dopo aver valutato le metriche durante la fase di allenamento (training loss, validation loss e Accuracy), si procede con la valutazione sul set di test per capire se il modello generalizza bene su dati mai visti, simulando l’uso reale.
"""

# Valutazione automatica con Trainer
test_metrics = trainer.evaluate(eval_dataset=test_dataset)
print("Valutazione automatica sul test set:")
print(test_metrics)

"""**STEP 5:** Analisi degli errori o anomalie

Questo step mi aiuta a comprendere dove il modello sbaglia e perché, per migliorarlo o interpretarlo meglio.
"""

from sklearn.metrics import classification_report


preds_output=trainer.predict(test_dataset)
preds=np.argmax(preds_output.predictions,axis=1)
labels=preds_output.label_ids

print(classification_report(preds,labels))

"""- Accuracy = 0.58 → il modello classifica correttamente il 58% delle recensioni.

- Macro avg (F1 = 0.46) → media semplice tra le classi, mostra che le classi meno rappresentate hanno performance più basse.

- Weighted avg (F1 = 0.54) → media ponderata per supporto: il modello è influenzato positivamente dalla classe 4, che ha il maggior numero di esempi.
"""

#salvo il modello per poi testarlo con la pipeline
trainer.save_model("best_rating_classifier")

"""**Interpretazione del report della classificazione:**


- Il modello funziona bene per le recensioni a 5 stelle, ma fatica con le classi intermedie (1, 2 ,3 stelle).

- Questo è coerente con la distribuzione sbilanciata del dataset: la classe 4 ha molti più esempi, quindi il modello la impara meglio.

- Le classi 1, 2 e 3 hanno recall molto basso, quindi il modello tende a non riconoscerle quando appaiono.

**Metriche Globali:**
- Accuracy = 0.58 → il modello classifica correttamente il 58% delle recensioni.

- Macro avg (F1 = 0.46) → media semplice tra le classi, mostra che le classi meno rappresentate hanno performance più basse.

- Weighted avg (F1 = 0.54) → media ponderata per supporto: il modello è influenzato positivamente dalla classe 4, che ha il maggior numero di esempi.

**Commento Generale:** "Il modello generalizza bene sulle recensioni a 5 stelle, con un recall del 91%. Tuttavia, mostra difficoltà nel riconoscere le classi intermedie, in particolare le recensioni a 2 e 3 stelle. Questo è dovuto allo sbilanciamento del dataset, che ho rispettato nel test set per garantire una valutazione realistica. L'accuracy complessiva è del 58%, e il F1 ponderato è 0.54, che considero una base solida per miglioramenti futuri."

#**FASE 3:POST-PROCESSING**
Adesso si procede con fase di post-processing:

1. Inferenza con Hugging Face Pipeline

2. Utilizzo la funzione Softmax per visualizzare tutte le probabilità



------------------------------
**STEP 1:** Inferenza con Hugging Face Pipeline

Qui, utilizzo il modello fine-tuning per classificare una recensione reale. Viene data in input una frase, e  restituisce il voto stimato (da 1 a 5 stelle) insieme a quanto è sicuro della scelta. È il primo modo per vedere il modello “in azione”, come se fosse già integrato in un’applicazione.
"""

from transformers import pipeline, AutoTokenizer
import torch

# Carico il tokenizer e il modello fine-tuned
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
model.eval()  # metto il modello in modalità valutazione

# Creo la pipeline di inferenza
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, top_k=None)

# Funzione per eseguire inferenza su un esempio del test set
def print_test_prediction(idx):
    comment = test_bal.iloc[idx]["comment"]
    true_label = test_bal.iloc[idx]["label"]

    # Eseguo la predizione
    results = classifier(comment, truncation=True, max_length=256)

    # Trovo la classe con il punteggio più alto
    best = max(results[0], key=lambda x: x["score"])
    predicted_label = int(best["label"].split("_")[-1])

    print(f"\n>>> Recensione:\n{comment[:300]}...")
    print(f">>> Etichetta vera: {true_label}")
    print(f">>> Predizione del modello: {predicted_label}")
    print(f">>> Confidenza: {best['score']:.4f}")


for i in range(20):
    print_test_prediction(i)

"""**INTERPRETAZIONE OUTPUT:**

- Predizioni corrette: Il modello predice correttamente molte recensioni con classe 4 (5 stelle), spesso con confidenza alta (>0.90). Questo conferma che la classe dominante è ben appresa.

- Errori su classi intermedie: alcune recensioni con etichetta 3 o 2 vengono classificate come 2 o 4, con confidenza moderata. Questo riflette la difficoltà del modello nel distinguere le sfumature tra classi vicine.

- Confidenza alta anche negli errori: in alcuni casi (es. etichetta vera 4 → predizione 0), il modello è molto sicuro di una predizione sbagliata. Questo è tipico nei modelli addestrati su dati sbilanciati.

**STEP 2:** Softmax per visualizzare tutte le probabilità

Qui si va più a fondo: invece di vedere solo il voto finale, chiedo al modello di mostrare quanto crede in ogni possibile voto. Ottiengo le percentuali per tutte le 5 classi, così posso capire se la scelta è netta o se il modello era indeciso tra due voti vicini. È utile per analizzare meglio il comportamento del modello e per rendere le sue decisioni più trasparenti.
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax
import torch
import pandas as pd

# Assicurati che il modello sia in modalità valutazione
model.eval()

# Lista dei risultati
results = []

# Ciclo su commenti e label veri
for comment, label in zip(test_bal["comment"], test_bal["label"]):
    # Tokenizzazione e invio alla stessa device del modello
    inputs = tokenizer(comment, return_tensors="pt", truncation=True, padding="max_length", max_length=256)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # Predizione
    with torch.no_grad():
        logits = model(**inputs).logits

    # Softmax e classe predetta
    probs = softmax(logits.cpu().numpy(), axis=1)[0]
    predicted_class = int(probs.argmax()) + 1
    true_class = int(label) + 1

    # Salvo tutto
    results.append([comment, predicted_class, [round(p, 4) for p in probs], true_class])

# Creo il DataFrame finale
df_results = pd.DataFrame(results, columns=["comment", "predicted_class", "probs", "true_class"])
df_results.head(20)

accuracy = (df_results["predicted_class"] == df_results["true_class"]).mean()
print(f"Accuracy finale sul test set: {accuracy:.4f}")

"""- Il modello funziona bene sulle classi 5 e 1, con confidenze alte e predizioni corrette.

- Fa errori gravi su alcune recensioni a 5 stelle, predicendo 1 con altissima sicurezza (riga 4).

- Le classi intermedie (3 e 4) sono più difficili da distinguere: il modello sbaglia spesso tra 3 e 4.

- La confidenza non sempre corrisponde alla correttezza: riga 2 e riga 4.

**Dove sbaglia?**
- Classe 5 predetta come 4 (es. riga 2, 14, 18): il modello confonde recensioni molto positive con quelle solo “buone”.

- Classe 5 predetta come 1 (riga 4): errore grave con confidenza altissima → segnale di overconfidence.

- Classe 2 predetta come 1 (riga 16): errore su classe intermedia, coerente con le difficoltà viste nel report F1.
"""